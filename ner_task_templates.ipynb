{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from codecs import xmlcharrefreplace_errors\n",
    "!pip install Faker\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install openai\n",
    "!pip install datasets"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from faker import Faker \n",
    "import itertools\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import openai \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, ClassLabel, load_from_disk\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ],
   "id": "ee94835a58bd096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "load_dotenv()",
   "id": "834089c67c927dcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "update_val_test = False\n",
    "TRAIN_DATASET = 'filtered_dataset_train_modif_2'\n",
    "OPENAI_API_KEY = os.getenv('OPEN_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY"
   ],
   "id": "b8d1b74b5db4550"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and Prepare the Data for the Templates",
   "id": "7be66ab029f02d9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"Read the Parquet file with the data tor the templates and convert it to a pandas DataFrame\n",
    "train_df = spark.read.parquet('ner_research/sample').toPandas()"
   ],
   "id": "a9eda8f7c3ae409c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove any leading '+' characters from the 'phone' column in the DataFrame to generalize better how the phones are presented in the templates\n",
    "train_df['phone'] = train_df['phone'].apply(lambda x: x.lstrip('+'))"
   ],
   "id": "218606b355ed8094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\" Create a new 'webpage' column by removing spaces and converting the orpanization column to lowercase to add webpages in the templates\n",
    "train_df['webpage'] = train_df['organization'].apply(lambda x: x. replace(\" \",\n",
    "\"\").lower())"
   ],
   "id": "79d5b0b64e9555b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_df['social_networks'] = train_df['email'].apply(lambda x: x. split(\"@\") [0].lower ())",
   "id": "89a168e63553d512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize a Faker obiect to generate fake data\n",
    "fake = Faker()"
   ],
   "id": "5b7e57bf15cfe62d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_spaces_conditionally(x, index, condition) :\n",
    "    \"\"\"\n",
    "    Remove spaces from a string based on a condition applied to its index.\n",
    "    \n",
    "    Args:\n",
    "    x (str): The input string.\n",
    "    index (int): The index of the input string.\n",
    "    condition (callable): A function to apply the condition on the index.\n",
    "    \n",
    "    Returns:\n",
    "    str: The modified string with spaces removed if the condition is met, otherwise the original string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if condition(index) :\n",
    "        return x.replace(\" \", \"\")\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def change_location_conditionally(x, index, condition):\n",
    "    \"\"\"\"\n",
    "    Change a location string based on a condition applied to its index.\n",
    "    Args:\n",
    "    x (str): The input location string.\n",
    "    index (int): The index of the input\n",
    "    condition (callable): A function to apply the condition on the index.\n",
    "\n",
    "    Returns:\n",
    "    str: A fake address if the condition is met, otherwise the original location string.\n",
    "    \"\"\"\n",
    "\n",
    "    if condition (index) :\n",
    "        return fake.address ()\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def condition(index):\n",
    "    \"\"\"\"\n",
    "    Check if the given index is even.\n",
    "    \n",
    "    Args:\n",
    "    index (int): The index to check.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the index is even, False otherwise.\n",
    "    \"\"\"\n",
    "    return index % 2 == 0"
   ],
   "id": "cf8047a27c0be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Update the 'phone' column by removing spaces conditionally using the 'condition' function to generalize better how the phones are presented in the templates\n",
    "train_df['phone'] = train_df.apply(lambda row: remove_spaces_conditionally(row['phone'], row.name, condition), axis=1)"
   ],
   "id": "10000e952af24b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Update the 'location' column by changing the location conditionally using the 'condition' function to generalize better how the locations are presented in the templates\n",
    "train_df['location'] = train_df.apply(lambda row: change_location_conditionally(row[' location'], row.name, condition), axis=1)"
   ],
   "id": "e3bbb973b42e02c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_df",
   "id": "10bf5d2a307622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the number of rows you want in the Validation and Test DataFrame\n",
    "NUM_SAMPLES = 1000\n",
    "# Generate the data\n",
    "data = {'name': [fake.name() for _ in range (NUM_SAMPLES)] ,\n",
    "        'job_title': [fake.job() for _ in range (NUM_SAMPLES)] ,\n",
    "        'organization': [fake.company() for _ in range(NUM_SAMPLES)] ,\n",
    "        'location': [fake.address() for _ in range(NUM_SAMPLES)]}\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "val_test_df = pd.DataFrame(data)"
   ],
   "id": "12429aca2134fa48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate the templates for the training Set",
   "id": "351e0d3da9206b7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_CLOSINGS = 15\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-4',\n",
    "    messages=[{\"role\": \"user\", \"content\": f'Generate {NUM_CLOSINGS} examples of messages that typically appear after an email signature. Include elements such as disclaimers, reminders, postscripts (P.S.), inspirational messages, important dates, and other types of notices.'}],\n",
    "    max_tokens=4000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "#Get the response content\n",
    "closing_text = response['choice'][0]['message']['content']"
   ],
   "id": "74fc577d817284e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "closings = [re.sub(r'^\\d+\\.\\s+', '', message.strip()) for message in re.split(r\"\\n\\n\\d+\\.\", closing_text) if message.strip()]",
   "id": "45dcc44adf17144e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_SPAMS = 40\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-4',\n",
    "    messages=[{\"role\": \"user\", \"content\": f'Generate a list of {NUM_SPAMS} creative and diverse spam email signatures from various fields and interests. Incorporating elements commonly found in spam emails, such as promotions, URLs, dollar amounts, dates, questions, subscriptions, unsubscriptions, access denials, daily reviews, Linkedin job suggestions, and more, using fictitious information. Start some of the messages with the organization name.'}],\n",
    "    max_tokens=8000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1.1\n",
    ")\n",
    "\n",
    "spam_text = response['choices'][0]['message']['content']"
   ],
   "id": "e56d820a434daed9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "spams = [re.sub(r'^\\d+\\.\\s+', '', message.strip()) for message in re.split(r\"\\n\\d+\\.\", spam_text) if message.strip()]",
   "id": "27c9323e68c932a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_sometimes(attribute, threshold, new_line=True):\n",
    "    #Generate a random number between 0 and 1\n",
    "    probability = random.random()\n",
    "\n",
    "    # Print the message if the random number is greater than the threshold\n",
    "    if probability > threshold:\n",
    "        if new_line:\n",
    "            return f'{attribute}\\n'\n",
    "        else:\n",
    "            return attribute\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "openings = [\"Best regards,\",\n",
    "            \"Kind regards,\",\n",
    "            \"Sincerely,\",\n",
    "            \"Best,\",\n",
    "            \"Warm regards,\",\n",
    "            \"Personal contact information\"]\n",
    "\n",
    "name_prefix = lambda: f\"{fake.prefix()} \"\n",
    "\n",
    "name = [f\"{print_sometimes('Name:', 0.9, False)}\"+print_sometimes('{name}', 0.05, False)]\n",
    "\n",
    "name_suffix = [\" (she/her)\", \" (he/him)\", lambda: f\" {fake.suffix()}\"]\n",
    "\n",
    "title_section = f\"{print_sometimes('Job Title: ', 0.9, False)}\"+\"{job_title}\"+f\"{print_sometimes ('Department', 0.9, False)}\"\n",
    "\n",
    "organization_section = \"{organization}\"\n",
    "\n",
    "location_section = f\"{print_sometimes ('Address: ', 0.9, False)}\"+\"{location}\"+f\"{print_sometimes(', ', 0.5, False)}\"+f\"{print_sometimes({fake.country()}, 0.5, False) }\"\n",
    "\n",
    "email_section = [\"Email: {email}\",\n",
    "                 \"{email}\"]\n",
    "\n",
    "phone_section = [f\"Phone{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Telephone{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Contact{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Direct{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Reception{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Office{print_sometimes(':', 0.5, False)} {print_sometimes ('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Call{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"Mobile{print_sometimes(':', 0.5, False)} {print_sometimes('+', 0.5, False)}\"+\"{phone}\",\n",
    "                 f\"{print_sometimes('+', 0.5, False)}\"+\"{phone}\"]\n",
    "\n",
    "webpage_section = [\"{webpage}/\"+f\"{fake.dga()}\"+\"fkedpem0m-2f-2fbglbdaiofac0jfz\",\n",
    "                   \"www.{webpage}/\"+f\"{fake.dga()}\"+\"byb21\",\n",
    "                   \"http://{webpage}/\"+f\"{fake.dga()}\",\n",
    "                   \"https://{webpage}/\"+f\"{fake.dga()}\"]\n",
    "\n",
    "closings = ['sent from outlook for ios',\n",
    "            f\"work days: {fake.day_of_week()}-{fake.day_of_week()} {fake.time('%H:%M')}am-{fake.time('%:%M')}pm\",\n",
    "            'all rights reserved',\n",
    "            f\"registered in {fake.countr()}\"]+closings\n",
    "\n",
    "social_networks = [f\"{print_sometimes('LinkedIn: ', 0.5, False)}\"+\"linkedin.com/in/{social_networks}\",\n",
    "                   f\"{print_sometimes('Twitter: ', 0.5, False)}\"+\"twitter.com/{social_networks}\",\n",
    "                   f\"{print_sometimes('Skype live: ', 0.5, False)}\"+\"{social_networks}\"]\n",
    "\n",
    "attribute_sections = [title_section,\n",
    "                      organization_section,\n",
    "                      location_section]\n",
    "\n",
    "separators = [\"\\n\", \", \", \" - \", \" | \", \" \"]\n",
    "\n",
    "# Generate combinations of attributes (title, organization, location) and combinations of email and phone sections\n",
    "attribute_combinations = list(set([\n",
    "    sep.join(perm)\n",
    "    for n in range(len(attribute_sections) + 1)\n",
    "    for subset in itertools.combinations(attribute_sections, n)\n",
    "    for perm in itertools.permutations(subset)\n",
    "    for sep in separators\n",
    "]))\n",
    "\n",
    "email_phone_combinations = list(set([\n",
    "    email + (sep if email and phone else \"\") + phone\n",
    "    for email, phone in itertools.product(email_section, phone_section)\n",
    "    for sep in separators\n",
    "]))+[\"\"]\n",
    "\n",
    "# Generate various email signature templates using combinations of different sections (salutations, name, attributes, email_phone, webpage, name_noises, and closing_noises)\n",
    "normal_templates = list(set([\n",
    "    f\"{print_sometimes(opening, 0.1)}\"\n",
    "    f\"{print_sometimes('', 0.9)}\"\n",
    "    f\"{(print_sometimes(name_prefix(), 0.6, False) if name else '')}\"\n",
    "    f\"{name}\"\n",
    "    f\"{(print_sometimes(',', 0.3, False) if name else '')}\"\n",
    "    f\"{(print_sometimes(name_suf() if callable(name_suf) else name_suf, 0.6, False) if name else '')}\"\n",
    "    f\"{(print_sometimes('', 0) if name else '')}\"\n",
    "    f\"{attributes}\"\n",
    "    f\"{(print_sometimes('', 0) if attributes else '')}\"\n",
    "    f\"{email_phone}\"\n",
    "    f\"{(print_sometimes('', 0) if email_phone else '')}\"\n",
    "    f\"{print_sometimes(webpage, 0.1)}\" \n",
    "    f\"{print_sometimes(social_network, 0.1)}\"\n",
    "    f\"{print_sometimes (closing,0.1, False)}\"\n",
    "    for opening,name, attributes,email_phone, webpage, name_suf, social_network, closing \n",
    "    in itertools.product(openings, name, attribute_combinations, email_phone_combinations, webpage_section, name_suffix, social_networks, closings)\n",
    "]))"
   ],
   "id": "3a87f4ea46c6ce7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate the total number of unique email signature templates generated\n",
    "len(normal_templates+spams)"
   ],
   "id": "b313a8c975281672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "select_normal = random.sample(normal_templates, train_df.shape[0])\n",
    "templates = select_normal+spams\n",
    "num_templates = train_df.shape[0]+NUM_SPAMS\n",
    "train = train_df.reindex(range(num_templates), fill_value='nan')\n",
    "train['template'] = templates"
   ],
   "id": "2d7c70cbb67172b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print the first 10 email signature templates for demonstration purposes\n",
    "for template in train.sample(50)['template']:\n",
    "    print(f'(template)\\n')"
   ],
   "id": "4f85d4be129d192d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "PROPORTION_SPAMS = 0.05\n",
    "num_spam = int(PROPORTION_SPAMS*NUM_SAMPLES)\n",
    "\n",
    "def generate_email_signature(batch_size):\n",
    "    \n",
    "    name = list(val_test_df['name'])\n",
    "    job_title = list(val_test_df['job_title'])\n",
    "    organization = list(val_test_df['organization'])\n",
    "    location = list(val_test_df['location'])\n",
    "    num_batches = math.ceil(NUM_SAMPLES / batch_size)\n",
    "    Signatures = list()\n",
    "    Spams = list()\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Generating signatures\"):\n",
    "        start_index = i * batch_size\n",
    "        end_index = min((i + 1) * batch_size, NUM_SAMPLES)\n",
    "        batch_names = name[start_index:end_index]\n",
    "        batch_job_title = job_title[start_index:end_index]\n",
    "        batch_organization = organization[start_index:end_index]\n",
    "        batch_location = location[start_index:end_index]\n",
    "        \n",
    "        prompt = (\n",
    "        f'Generate {batch_size} diverse and realistic text email signature templates. For each template, use the corresponding elements from the {batch_names} (for names), {batch_job_title} (for job titles), {batch_organization} (for company names), and {batch_location} (for addresses) lists based on their indices (e.g., use the first elements from each list for the first template, the second elements for the second template, and so on). It is imperative to use the elements from {batch_names}, {batch_job_title}, {batch_organization}, and {batch_location} precisely as they are given, without any alteration. Do not modify, adjust, or correct the elements in any way, including not capitalizing, not changing spacing, not changing or adding any type of punctuation, word order, or formatting. Preserve the original state of each element unconditionally. For example, if the given company name is \"Ballard, Williams and Smith,\" use it as is, without changing it to \"Ballard, Williams, and Smith.\" If the job title is \"Diagnostic radiographer,\" leave it like that and do not change it to \"Diagnostic Radiographer.\" Please respect this for all templates. In the templates, vary the order of elements in a realistic manner. Ensure that all variations make sense and are consistent with real-world email signatures. In addition to the provided elements, incorporate a variety of common email signature elements, generating fictional information for each, in no particular order, including but not limited to:'\n",
    "        f'-Occasionally, a message before the signature'\n",
    "        f'-Name followed by titles, pronouns, certifications, or awards'\n",
    "        f'-Department names'\n",
    "        f'-Multiple phone numbers and contact details, including office, mobile, and fax numbers'\n",
    "        f'-Email addresses'\n",
    "        f'-Social media links (LinkedIn, Twitter, etc.)'\n",
    "        f'-Company websites and personal websites or blogs'\n",
    "        f'-Disclaimers and confidentiality statements'\n",
    "        f'-Legal or regulatory information'\n",
    "        f'-Slogans, quotes, or inspirational messages!'\n",
    "        f'Include region-specific signatures, and represent QR codes as URLs.'\n",
    "        f'Vary the presentation of the additional elements, sometimes including only a subset of them or using different formats and arrangements for a more diverse and realistic set of templates.'\n",
    "        f'Present the templates consistently, separated as follows:'\n",
    "        f'Template 1:'\n",
    "        f'Template 2:'\n",
    "        f'And so on, ensuring clear separation between templates.\"\n",
    "    )\n",
    "        \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=4500,\n",
    "        n=1,\n",
    "        Stop=None,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    # Get the response content\n",
    "    response_text = response[' choices'][0]['message ']['content']\n",
    "    Signatures.extend([template.strip() for template in re.split(r\"Template\\s+\\d+:\", response_text) if template.strip()])\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate a list of (num_spam) diverse spam email signatures without including name, job title, company name, or address. Incorporate elements typically found in spam, such as promotions and URLs, with fake information.\"\n",
    "        f\"Present the templates consistently, separated as follows: \"\n",
    "        f\"Template 1:\"\n",
    "        f\"Template 2:\"\n",
    "        f\"And so on, ensuring clear separation between templates.\"\n",
    "    )\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1500,\n",
    "        n = 1,\n",
    "        Stop=None,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    Spams.extend([template.strip() for template in re.split(r\"Template\\s+\\d+:\", response_text) if template.strip()])\n",
    "    \n",
    "    return Signatures+Spams"
   ],
   "id": "bf3dfe188542c51b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if update_val_test:\n",
    "    signatures = generate_email_signature(20)\n",
    "    # Add generated_signatures as a new column and add 'NA' or NaN to extra rows in other columns\n",
    "    num_signatures = NUM_SAMPLES+num_spam-1\n",
    "    val_test_df = val_test_df.reindex(range(num_signatures), fill_value='nan')\n",
    "    val_test_df['signature'] = signatures\n",
    "\n",
    "    val_test_data = val_test_df"
   ],
   "id": "fb119f1f31dbeba5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Dataset objects for Training, Validation and Test Set",
   "id": "da3821c33e2ed755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the NLP spacy \"en_core_web_sm\" model and define a function to process email signatures\n",
    "# The function assigns NER tags (PER, JOB, ORG, LOC) to the relevant sections in the signature\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "\n",
    "def process_email_signature(row, train):\n",
    "    if train:\n",
    "        signature = row['template'].format (**row)\n",
    "    else:\n",
    "    signature = row['signature']\n",
    "\n",
    "    doc = nlp(signature.lower())\n",
    "    tokens = [token.text for token in doc]\n",
    "    ner_tags = ['O'] * len (tokens)\n",
    "\n",
    "    def assign_tags(entity, tag):\n",
    "        entity_tokens = [token.text for token in nlp(entity)]\n",
    "        n = len(entity_tokens)\n",
    "\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            if tokens[i:i+n] == entity_tokens:\n",
    "                for j in range(n):\n",
    "                    if j == 0:\n",
    "                        ner_tags[i] = f'B-{tag}'\n",
    "                    else:\n",
    "                        ner_tags[i + j] = f'I-{tag}'\n",
    "\n",
    "    assign_tags(row['name'].lower(), 'PER')\n",
    "    assign_tags(row['job_title'].lower(), 'JOB')\n",
    "    assign_tags(row['organization'].lower(), 'ORG')\n",
    "    assign_tags(row['location'].lower(), 'LOC')\n",
    "    \n",
    "    return tokens, ner_tags\n",
    "\n",
    "# Define a function to create a dataset for NER training and evaluation from the input data\n",
    "# The function processes email signatures, tokenize the text and assign NER tags (PER, JOB, ORG, LOC) to the relevant sections\n",
    "# It then splits the data into train, validation, and test sets, and creates Dataset objects for each set\n",
    "\n",
    "def create_dataset(train, val_test):\n",
    "    # Process the email signatures in the input data using the process_email_signature function\n",
    "    # For each email signature, tokenize the text and assign NER tags (PER, JOB, ORG, LOC) to the relevant sections\n",
    "    train_results = [process_email_signature(row, train=True) for row in tqdm(train.to_dict('records'), desc=\"Processing email signatures\")]\n",
    "    train_tokenized_signatures, train_ner_tags = zip(*train_results)\n",
    "    \n",
    "    if update_val_test:\n",
    "        val_test_results = [process_email_signature(row, train=False) for row in tqdm(val_test.to_dict('records'), desc=\"Processing email signatures\")]\n",
    "        val_test_tokenized_signatures, val_test_ner_tags = zip (*val_test_results)\n",
    "        \n",
    "    tag_mapping = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-JOB': 3, 'I-JOB': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8}\n",
    "    \n",
    "    def convert_tags(tags):\n",
    "        return[tag_mapping[tag] for tag in tags]\n",
    "\n",
    "    features = Features ({\n",
    "    'tokens': Sequence(Value(dtype='string', id=None)),\n",
    "    'ner_tags': Sequence(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-JOB', 'I-JOB', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None))\n",
    "    })\n",
    "\n",
    "    train_ner_tags_int = [convert_tags(signature_tags) for signature_tags in train_ner_tags]\n",
    "    train_data = [{'tokens': signature, 'ner_tags': signature_tags} for signature, signature_tags in zip(train_tokenized_signatures, train_ner_tags_int)]\n",
    "    train_dataset = Dataset.from_dict({k: [d[k] for d in train_data] for k in train_data[0]}, features=features)\n",
    "\n",
    "    if update_val_test:\n",
    "        val_test_ner_tags_int = [convert_tags(signature_tags) for signature_tags in val_test_ner_tags]\n",
    "        val_test_data = [{'tokens': signature, 'ner_tags': signature_tags} for signature, signature_tags in zip(val_test_tokenized_signatures, val_test_ner_tags_int)]\n",
    "        val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "        val_dataset = Dataset.from_dict({k: [d[k] for d in val_data] for k in val_data[0]}, features=features)\n",
    "        test_dataset = Dataset.from_dict({k: [d[k] for d in test_data] for k in test_data[0]}, features=features)\n",
    "\n",
    "        # Create a DatasetDict containing the train, validation, and Test Datasets\n",
    "        # Each dataset in the dictionary consists of tokens and NER tags, which will be used for model training and evaluation\n",
    "\n",
    "        dataset_dict = DatasetDict({\n",
    "        'train' : train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset,\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        dataset_dict = load_from_disk('ner_research/filtered_dataset')\n",
    "        dataset_dict['train'] = train_dataset\n",
    "    \n",
    "    return dataset_dict"
   ],
   "id": "f83a3fa220eaf8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if update_val_test:\n",
    "    dataset_dict = create_dataset(train, val_test)\n",
    "else:\n",
    "    dataset_dict = create_dataset(train, '')"
   ],
   "id": "d2da37c3e32e508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "filtered_validation = dataset_dict['validation'].select([i for i in range(dataset_dict['validation'].num_rows) if (set(range(9)) == set(dataset_dict['validation'][i]['ner_tags'])) or (set(range(1)) == set(dataset_dict['validation'][i]['ner_tags']))])\n",
    "\n",
    "filtered_test = dataset_dict['test'].select([i for i in range(dataset_dict['test'].num_rows) if (set(range(9)) == set(dataset_dict['test'][i]['ner_tags'])) or (set(range(1)) == set(dataset_dict['test'][i]['ner_tags']))])\n",
    "\n",
    "#Update the 'validation' key in dataset_dict with the filtered dataset\n",
    "dataset_dict['validation'] = filtered_validation\n",
    "dataset_dict['test'] = filtered_test"
   ],
   "id": "8e52be715833e596"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filtered_dataset_dict = dataset_dict",
   "id": "21b28066b5e4e703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filtered_dataset_dict.save_to_disk(f\"ner_research/{TRAIN_DATASET}/\")",
   "id": "f0594f2f2b420608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25ace4c478632a2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
