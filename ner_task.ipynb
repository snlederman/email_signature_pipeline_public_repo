{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "pip install datasets"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk"
   ],
   "id": "798e2349f02fd637"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL_NAME = 'updated_roberta_1k_2'\n",
    "OUTPUT_DIR = f'ner_research/models/{MODEL_NAME}'\n",
    "pipe = pipeline('ner', model=OUTPUT_DIR, tokenizer='roberta-base')"
   ],
   "id": "8131d8a989976813"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "color_map = {0: 'black',\n",
    "             1: 'red',\n",
    "             2: 'red',\n",
    "             3: 'blue',\n",
    "             4: 'blue',\n",
    "             5: 'green',\n",
    "             6: 'green',\n",
    "             7: 'purple',\n",
    "             8: 'purple'}"
   ],
   "id": "74bf85a07c101c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def ner_tokens_to_html(tokens):\n",
    "    html_code = \"\"\n",
    "    previous_color = None\n",
    "    for string, color in tokens:\n",
    "        if string=='\\n':\n",
    "            string = '<br>'\n",
    "        if color!=previous_color:\n",
    "            if previous_color is not None:\n",
    "                html_code += '</span>'\n",
    "            html_code += f\"<span style='color: {color_map[color]};'>{string}\"\n",
    "        else:\n",
    "            html_code += string\n",
    "        previous_color = color\n",
    "    html_code += '</span>'\n",
    "    return html_code"
   ],
   "id": "c8ba6493bc8959c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TRAIN_DATASET = 'filtered_dataset_train_modif_2'\n",
    "dataset_dict = load_from_disk(f'ner_research/{TRAIN_DATASET}/')"
   ],
   "id": "df250a71f70c94d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "samples = np.random.randint(0, dataset_dict['test'].num_rows, size=5)",
   "id": "8a1b513ba52f183"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for j in samples:\n",
    "    j = int(j)\n",
    "    tokens = [(dataset_dict['test'][i]['tokens'][i], dataset_dict['test'][i]['ner_tags'][i]) for i in range(len(dataset_dict['test'][j]['tokens']))]\n",
    "    displayHTML(ner_tokens_to_html(tokens))"
   ],
   "id": "c35981f0c93fc3bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in samples:\n",
    "    i = int(i)\n",
    "    text= ' '.join(dataset_dict['test'][i]['tokens'])\n",
    "    input_data = pipe.tokenizer(text, return_tensors='tf', truncation=True)\n",
    "    output_data = pipe.model(**input_data)\n",
    "    tokens = [(pipe.tokenizer.decode(input_data['input_ids'][0][i]), np.argmax(output_data['logits'], axis=2)[0][i]) for i in range(output_data['logits'].shape[1])]\n",
    "    displayHTML(ner_tokens_to_html(tokens[1:-1]))"
   ],
   "id": "efdbe352020e5c4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_parquet('/dbfs/mnt/ds-prod-assets/community_signatures/outputs/temp_1M_signatures_df.parquet')\n",
    "has_details_filter = (data['emails'].apply(len).gt(0) |\n",
    "                      data['phones'].apply(len).gt(0) |\n",
    "                      data['address'].notnull() |\n",
    "                      data['company'].notnull() |\n",
    "                      datal['job_titles'].apply(len).gt(0) |\n",
    "                      data['socials'].apply(len).gt(0))\n",
    "num_lines_filter = (data['end_pos'] - data['start_pos']).between(4, 10)\n",
    "data = data[num_lines_filter & has_details_filter]\n",
    "data = data.drop_duplicates(subset='body', keep='first')\n",
    "sample = data[:25]"
   ],
   "id": "c422fdf5957aa63d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(sample.shape[0]):\n",
    "    text= sample['body'].iat[i]\n",
    "    input_data = pipe.tokenizer(text, return_tensors='tf', truncation=True)\n",
    "    output_data = pipe.model(**input_data)\n",
    "    tokens = [(pipe.tokenizer.decode(input_data['input_ids'][0][i]), np.argmax(output_data['logits'], axis=2)[0][i]) for i in range(output_data['logits'].shape[1])]\n",
    "    displayHTML(ner_tokens_to_html(tokens[1:-1]))"
   ],
   "id": "f9f51b266a42a837"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
